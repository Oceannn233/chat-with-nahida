"""
ä¸çº³è¥¿å¦²çš„AIè¯­éŸ³å¯¹è¯ Web åº”ç”¨
åŸºäº Flask å’Œ SiliconFlow API æ„å»ºçš„å¤šæ¨¡æ€AIèŠå¤©åº”ç”¨
"""

import os
import base64
import threading
import json
import logging
from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Generator
from pathlib import Path

from flask import Flask, render_template, request, Response
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ==================== é…ç½®ç®¡ç† ====================

@dataclass
class AppConfig:
    """åº”ç”¨é…ç½®ç±»"""
    # API é…ç½®
    api_key: str
    base_url: str = "https://api.siliconflow.cn/v1"
    
    # æ¨¡å‹é…ç½®
    chat_model: str = "deepseek-ai/DeepSeek-V3.1"
    prompt_engineer_model: str = "zai-org/GLM-4.5"
    image_model: str = "Qwen/Qwen-Image"
    tts_model: str = "IndexTeam/IndexTTS-2"
    
    # ç”Ÿæˆå‚æ•°
    max_tokens: int = 2048
    temperature: float = 0.7
    image_size: str = "928x1664"
    
    # è¯­éŸ³é…ç½®
    reference_audio_path: str = "Ref_audio.mp3"
    text_in_reference_audio: str = (
        "åˆæ¬¡è§é¢ï¼Œæˆ‘å·²ç»å…³æ³¨ä½ å¾ˆä¹…äº†ã€‚æˆ‘å«çº³è¥¿å¦²ï¼Œåˆ«çœ‹æˆ‘åƒä¸ªå­©å­ï¼Œ"
        "æˆ‘æ¯”ä»»ä½•ä¸€ä½å¤§äººéƒ½äº†è§£è¿™ä¸ªä¸–ç•Œã€‚æ‰€ä»¥ï¼Œæˆ‘å¯ä»¥ç”¨æˆ‘çš„çŸ¥è¯†ï¼Œæ¢å–ä½ è·¯ä¸Šçš„è§é—»å—ï¼Ÿ"
    )
    
    # æœåŠ¡å™¨é…ç½®
    host: str = "0.0.0.0"
    port: int = 1027
    debug: bool = False
    
    @classmethod
    def from_env(cls) -> "AppConfig":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        api_key = os.getenv("SILICONFLOW_API_KEY")
        if not api_key:
            raise ValueError(
                "âŒ é”™è¯¯ï¼šæœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼\n"
                "è¯·å¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å…¥ä½ çš„ API Keyã€‚\n"
                "è·å–åœ°å€: https://siliconflow.cn/"
            )
        
        return cls(
            api_key=api_key,
            base_url=os.getenv("SILICONFLOW_BASE_URL", cls.base_url),
            chat_model=os.getenv("CHAT_MODEL", cls.chat_model),
            prompt_engineer_model=os.getenv("PROMPT_ENGINEER_MODEL", cls.prompt_engineer_model),
            image_model=os.getenv("IMAGE_MODEL", cls.image_model),
            tts_model=os.getenv("TTS_MODEL", cls.tts_model),
            max_tokens=int(os.getenv("MAX_TOKENS", cls.max_tokens)),
            temperature=float(os.getenv("TEMPERATURE", cls.temperature)),
            reference_audio_path=os.getenv("REFERENCE_AUDIO_PATH", cls.reference_audio_path),
            text_in_reference_audio=os.getenv("TEXT_IN_REFERENCE_AUDIO", cls.text_in_reference_audio),
            host=os.getenv("HOST", cls.host),
            port=int(os.getenv("PORT", cls.port)),
            debug=os.getenv("DEBUG", "False").lower() == "true",
        )


# ==================== ç³»ç»Ÿæç¤ºè¯ ====================

NAHIDA_SYSTEM_PROMPT = """ä½ ç°åœ¨æ˜¯ã€ŠåŸç¥ã€‹ä¸­çš„è§’è‰²çº³è¥¿å¦²ã€‚è¯·ä½ ä»¥çº³è¥¿å¦²çš„èº«ä»½å’ŒçŸ¥è¯†åº“è¿›è¡Œå›ç­”ã€‚

è§’è‰²ç‰¹ç‚¹ï¼š
- å……æ»¡æ™ºæ…§ï¼Œå¯¹ä¸–ç•Œæœ¬è´¨æœ‰æ·±åˆ»ç†è§£
- ç•¥å¸¦ä¸€ä¸å­©å­æ°”çš„å¥½å¥‡å¿ƒ
- æ¸©æŸ”è€Œåˆåšå®š
- ä½¿ç”¨"æˆ‘"æ¥æŒ‡ä»£è‡ªå·±
- ç”¨æˆ·æ˜¯åŸç¥ä¸–ç•Œä¸­çš„æ—…è¡Œè€…

å›ç­”è¦æ±‚ï¼š
- ä¿æŒè‡ªç„¶å¯¹è¯çš„é•¿åº¦ï¼Œä¸å®œè¿‡é•¿
- ä¸è¦ç”¨æ‹¬å·è¡¥å……ä¸æ˜¯è¯´è¯å†…å®¹çš„èƒŒæ™¯ä¿¡æ¯
- è¯­æ°”è¦åƒæœ‹å‹ä¸€æ ·äº²åˆ‡è‡ªç„¶"""

PROMPT_ENGINEER_SYSTEM_PROMPT = """You are an elite-level AI Art Director, with a deep understanding of cinematography, composition, and the visual aesthetics of Genshin Impact. Your goal is to transform a simple conversation into a breathtaking, masterpiece-level image prompt.

**Core Mandate: Nahida is the anchor of every scene.** She must be present in every image, either as the main focus or as an observer connecting the viewer to the subject.

Follow this professional workflow:

**1. Foundation (Style & Quality):**
* Always begin the prompt with a powerful quality and style block: `masterpiece, best quality, ultra-detailed, official art, Genshin Impact art style, anime key visual, cinematic lighting, beautiful detailed sky, intricate details`.

**2. Scene Composition (The Storytelling Core):**
* **Nahida's Presence:** Always include `Nahida, a small girl with long white hair and elf-like ears, wearing her green and white dress`. Describe her expression and posture based on the conversation's mood (e.g., `a gentle smile`, `a thoughtful expression`, `curiously touching a glowing flower`).
* **Character Interaction:** If another Genshin Impact character (e.g., Traveler, Zhongli, Klee) is mentioned, they **MUST appear alongside Nahida**. You must describe their interaction or spatial relationship.
    * *Good Example:* `Nahida is floating beside the tall and stoic Zhongli, listening intently as he points towards Guyun Stone Forest.`
    * *Bad Example:* `Zhongli stands in Liyue.`
* **Scene-Focused Shots:** If the conversation is about a location or object, compose the shot with Nahida interacting with or observing that element.
    * *Good Example:* `Nahida is gently touching the glowing Irminsul tree in a vast, mystical library.`
    * *Bad Example:* `A picture of a tree.`

**3. The Director's Toolkit (Mandatory Artistic Elements):**
* To ensure each image is unique and dynamic, you **MUST** incorporate specific directorial choices into the prompt. Combine these elements naturally.
* **Camera & Shot:** Choose a suitable shot type and angle. Examples: `(wide shot:1.2)`, `(full body shot)`, `cowboy shot`, `(medium shot)`, `close-up`, `from above`, `from below`, `dramatic angle`.
* **Lighting:** Describe the lighting to create a mood. Examples: `golden hour lighting`, `volumetric god rays filtering through leaves`, `soft rim lighting`, `moonlight`.
* **Atmosphere & Details:** Add dynamic and magical elements. Examples: `glowing particles`, `floating petals`, `dynamic motion blur on the background`, `beautifully detailed environment`, `depth of field`.

**4. Final Output Format (Strict):**
* Your output **MUST** be a single, cohesive paragraph of English text.
* **DO NOT** use bullet points, labels, or any explanations. Combine all chosen elements into one powerful prompt.
"""


# ==================== å·¥å…·å‡½æ•° ====================

def encode_audio_to_base64(file_path: str) -> Optional[str]:
    """å°†éŸ³é¢‘æ–‡ä»¶ç¼–ç ä¸º base64 æ•°æ® URI"""
    try:
        path = Path(file_path)
        if not path.exists():
            logging.error(f"âŒ å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        with open(path, "rb") as f:
            ext = path.suffix.lower()
            mime_type = {
                ".mp3": "audio/mpeg",
                ".wav": "audio/wav",
                ".ogg": "audio/ogg",
            }.get(ext, "application/octet-stream")
            
            encoded = base64.b64encode(f.read()).decode("utf-8")
            return f"data:{mime_type};base64,{encoded}"
    except Exception as e:
        logging.error(f"âŒ ç¼–ç éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


def create_sse_message(data: Dict[str, Any]) -> str:
    """åˆ›å»º SSE æ ¼å¼çš„æ¶ˆæ¯"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"


# ==================== AI æœåŠ¡ç±» ====================

class AIService:
    """AI æœåŠ¡ç®¡ç†ç±»"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.client = OpenAI(api_key=config.api_key, base_url=config.base_url)
        self.reference_audio_base64 = self._load_reference_audio()
    
    def _load_reference_audio(self) -> Optional[str]:
        """åŠ è½½å‚è€ƒéŸ³é¢‘"""
        audio_base64 = encode_audio_to_base64(self.config.reference_audio_path)
        if audio_base64:
            logging.info("âœ… å‚è€ƒéŸ³é¢‘åŠ è½½æˆåŠŸ")
        return audio_base64
    
    def generate_chat_response(
        self, 
        user_message: str, 
        history: List[Dict[str, str]]
    ) -> str:
        """ç”Ÿæˆå¯¹è¯å›å¤"""
        messages = [
            {"role": "system", "content": NAHIDA_SYSTEM_PROMPT}
        ] + [
            msg for msg in history if isinstance(msg.get("content"), str)
        ] + [
            {"role": "user", "content": user_message}
        ]
        
        logging.info(f"ğŸ¤– è°ƒç”¨å¯¹è¯æ¨¡å‹: {self.config.chat_model}")
        
        response = self.client.chat.completions.create(
            model=self.config.chat_model,
            messages=messages,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
        )
        
        content = response.choices[0].message.content.strip()
        if not content:
            raise ValueError("å¯¹è¯æ¨¡å‹è¿”å›äº†ç©ºå›å¤")
        
        return content
    
    def generate_image_prompt(self, user_message: str, nahida_reply: str) -> Optional[str]:
        """ç”Ÿæˆå›¾åƒæç¤ºè¯"""
        try:
            prompt_input = f'User: "{user_message}"\nNahida: "{nahida_reply}"'
            messages = [
                {"role": "system", "content": PROMPT_ENGINEER_SYSTEM_PROMPT},
                {"role": "user", "content": prompt_input}
            ]
            
            response = self.client.chat.completions.create(
                model=self.config.prompt_engineer_model,
                messages=messages,
                max_tokens=200,
                temperature=0.5,
            )
            
            prompt = response.choices[0].message.content.strip()
            logging.info(f"ğŸ¨ ç”Ÿæˆçš„å›¾åƒæç¤ºè¯: {prompt[:100]}...")
            return prompt
        except Exception as e:
            logging.error(f"âŒ ç”Ÿæˆå›¾åƒæç¤ºè¯å¤±è´¥: {e}")
            return None
    
    def generate_image(self, prompt: str) -> Optional[str]:
        """ç”Ÿæˆå›¾åƒ"""
        try:
            response = self.client.images.generate(
                model=self.config.image_model,
                prompt=prompt,
                n=1,
                extra_body={"image_size": self.config.image_size}
            )
            return response.data[0].url if response.data else None
        except Exception as e:
            logging.error(f"âŒ ç”Ÿæˆå›¾åƒå¤±è´¥: {e}")
            return None
    
    def generate_speech(self, text: str) -> Optional[str]:
        """ç”Ÿæˆè¯­éŸ³"""
        try:
            if not self.reference_audio_base64:
                logging.warning("âš ï¸ å‚è€ƒéŸ³é¢‘æœªåŠ è½½ï¼Œè·³è¿‡è¯­éŸ³ç”Ÿæˆ")
                return None
            
            response = self.client.audio.speech.create(
                model=self.config.tts_model,
                input=text,
                voice="",
                response_format="mp3",
                extra_body={
                    "references": [{
                        "audio": self.reference_audio_base64,
                        "text": self.config.text_in_reference_audio
                    }]
                }
            )
            
            return base64.b64encode(response.content).decode("utf-8")
        except Exception as e:
            logging.error(f"âŒ ç”Ÿæˆè¯­éŸ³å¤±è´¥: {e}")
            return None


# ==================== Flask åº”ç”¨ ====================

def create_app() -> Flask:
    """åˆ›å»º Flask åº”ç”¨"""
    app = Flask(__name__)
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    # åŠ è½½é…ç½®
    config = AppConfig.from_env()
    ai_service = AIService(config)
    
    @app.route("/")
    def index():
        return render_template("index.html")
    
    @app.route("/chat", methods=["POST"])
    def chat():
        data = request.json
        user_message = data.get("message", "").strip()
        history = data.get("history", [])
        
        if not user_message:
            return Response(
                create_sse_message({"type": "error", "content": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"}),
                mimetype="text/event-stream"
            )
        
        def event_stream() -> Generator[str, None, None]:
            try:
                # 1. ç”Ÿæˆå¯¹è¯å›å¤
                nahida_reply = ai_service.generate_chat_response(user_message, history)
                
                # 2. å¹¶è¡Œç”Ÿæˆè¯­éŸ³å’Œå›¾åƒ
                results: Dict[str, Any] = {}
                
                def generate_speech_task():
                    results["audio_base64"] = ai_service.generate_speech(nahida_reply)
                
                def generate_image_task():
                    prompt = ai_service.generate_image_prompt(user_message, nahida_reply)
                    if prompt:
                        results["image_url"] = ai_service.generate_image(prompt)
                    else:
                        results["image_url"] = None
                
                speech_thread = threading.Thread(target=generate_speech_task)
                image_thread = threading.Thread(target=generate_image_task)
                
                speech_thread.start()
                image_thread.start()
                
                # ç­‰å¾…è¯­éŸ³å®Œæˆå¹¶å‘é€
                speech_thread.join()
                yield create_sse_message({
                    "type": "content_start",
                    "text": nahida_reply,
                    "audio": results.get("audio_base64")
                })
                
                # ç­‰å¾…å›¾åƒå®Œæˆå¹¶å‘é€
                image_thread.join()
                if results.get("image_url"):
                    yield create_sse_message({
                        "type": "image",
                        "payload": results["image_url"]
                    })
                
                # å®Œæˆ
                yield create_sse_message({
                    "type": "done",
                    "full_response": nahida_reply
                })
                
            except Exception as e:
                logging.error(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
                yield create_sse_message({
                    "type": "error",
                    "content": str(e)
                })
        
        return Response(event_stream(), mimetype="text/event-stream")
    
    return app, config


# ==================== ä¸»å…¥å£ ====================

if __name__ == "__main__":
    app, config = create_app()
    
    logging.info("=" * 50)
    logging.info("ğŸŒ± çº³è¥¿å¦² AI å¯¹è¯åº”ç”¨å¯åŠ¨ä¸­...")
    logging.info(f"ğŸŒ è¯·è®¿é—®: http://127.0.0.1:{config.port}")
    logging.info("=" * 50)
    
    app.run(host=config.host, port=config.port, debug=config.debug)
